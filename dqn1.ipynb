{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP4wHJQOA0SnluwVRGeSouJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"VqoaOTUxAFHp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682214926091,"user_tz":-180,"elapsed":7,"user":{"displayName":"Iason-Dimitrios Kalampokidis","userId":"07271359253816325651"}},"outputId":"52b936c2-2561-440f-8174-38fe8c501405"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-2-a4d8f2c8fa73>:5: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n","  plt.style.use('seaborn-white')\n"]}],"source":["import math \n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","plt.style.use('seaborn-white')\n","import pandas as pd\n","from matplotlib import animation, rc\n","plt.rcParams.update({'pdf.fonttype': 'truetype'})\n","import math\n","import random\n","import numpy as np\n","from IPython.display import clear_output\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from collections import namedtuple\n","from itertools import count\n","import torch.nn.functional as F\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","\n","\n","from bokeh.io import show, output_notebook\n","from bokeh.palettes import PuBu4\n","from bokeh.plotting import figure\n","from bokeh.models import Label\n","\n","\n","\n","# Visualization functions\n","def plot_return_trace(returns,T, smoothing_window=10, range_std=2):\n","    plt.figure(figsize=(16, 5))\n","    plt.xlabel(\"Episode\")\n","    plt.ylabel(\"Return ($)\")\n","    returns_df = pd.Series(returns)\n","    ma = returns_df.rolling(window=smoothing_window).mean()\n","    mstd = returns_df.rolling(window=smoothing_window).std()\n","    plt.plot(ma, c = 'blue', alpha = 1.00, linewidth = 1)\n","    plt.fill_between(mstd.index, ma-range_std*mstd, ma+range_std*mstd, color='blue', alpha=0.2)\n","\n","def plot_price_schedules(p_trace, sampling_ratio, last_highlights,T):\n","    \n","    plt.xlabel(\"Time step\");\n","    plt.ylabel(\"Price ($)\");\n","    plt.xticks(range(T))\n","    plt.plot(range(T), np.array(p_trace[0:-1:sampling_ratio]).T, c = 'k', alpha = 0.05)\n","    return plt.plot(range(T), np.array(p_trace[-(last_highlights+1):-1]).T, c = 'red', alpha = 0.5, linewidth=2)\n","\n","def bullet_graph(data, labels=None, bar_label=None, axis_label=None,\n","                size=(5, 3), palette=None, bar_color=\"black\", label_color=\"gray\"):\n","    stack_data = np.stack(data[:,2])\n","\n","    cum_stack_data = np.cumsum(stack_data, axis=1)\n","    h = np.max(cum_stack_data) / 20\n","\n","    fig, axarr = plt.subplots(len(data), figsize=size, sharex=True)\n","\n","    for idx, item in enumerate(data):\n","\n","        if len(data) > 1:\n","            ax = axarr[idx]\n","\n","        ax.set_aspect('equal')\n","        ax.set_yticklabels([item[0]])\n","        ax.set_yticks([1])\n","        ax.spines['bottom'].set_visible(False)\n","        ax.spines['top'].set_visible(False)\n","        ax.spines['right'].set_visible(False)\n","        ax.spines['left'].set_visible(False)\n","\n","        prev_limit = 0\n","        for idx2, lim in enumerate(cum_stack_data[idx]):\n","            ax.barh([1], lim - prev_limit, left=prev_limit, height=h, color=palette[idx2])\n","            prev_limit = lim\n","        rects = ax.patches\n","        ax.barh([1], item[1], height=(h / 3), color=bar_color)\n","\n","    if labels is not None:\n","        for rect, label in zip(rects, labels):\n","            height = rect.get_height()\n","            ax.text(\n","                rect.get_x() + rect.get_width() / 2,\n","                -height * .4,\n","                label,\n","                ha='center',\n","                va='bottom',\n","                color=label_color)\n","            \n","    if bar_label is not None:\n","        rect = rects[0]\n","        height = rect.get_height()\n","        ax.text(\n","            rect.get_x() + rect.get_width(),\n","            -height * .1,\n","            bar_label,\n","            ha='center',\n","            va='bottom',\n","            color='white')\n","    if axis_label:\n","        ax.set_xlabel(axis_label)\n","    fig.subplots_adjust(hspace=0)\n","\n","\n","def predict(df,arima,elasticity,T,p_min,p_avg,cost,plot_graphs=False):\n","\n","  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","  Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n","\n","  #set price range that of last year \n","  min=cost \n","  max=df.price_per_unit[len(df)-52:].max()\n","  \n","\n","\n","  price_grid = np.arange(min, max,(max-min)/20)\n","  #unit_cost=0.8*min ##########################################################################################prososxii na rwtisw vavliaki\n","  unit_cost=cost\n","  forecasts=arima[:T]\n","  forecasts=forecasts.values.tolist()\n","  \n","  def profit_t_response(p_t, p_t_1,min,avg):\n","    return profit_t(p_t, p_t_1, elasticity,forecasted_sales_t,min,avg)\n","\n","  def profit_t(p_t, p_t_1, elsticity,forecasted_sales_t,p_cheapest,p_average):\n","      #print(p_t,p_t-1,q_t(p_t, p_t_1,elasticity,forecasted_sales_t),(p_t - unit_cost) )\n","      #discount_factor=(p_t-p_t_1)*forecasted_sales_t*elasticity/p_t_1\n","      cheapest_factor=(p_t-p_cheapest)*forecasted_sales_t*elasticity/p_cheapest\n","      average_factor=(p_t-p_average)*forecasted_sales_t*elasticity/p_average\n","      total_sales=forecasted_sales_t+cheapest_factor+average_factor\n","      if total_sales<0:\n","        total_sales=0\n","\n","      return (p_t - unit_cost)*total_sales\n","\n","\n","\n","  # A cyclic buffer of bounded size that holds the transitions observed recently\n","  class ReplayMemory(object):\n","      def __init__(self, capacity):\n","          self.capacity = capacity\n","          self.memory = []\n","          self.position = 0\n","\n","      def push(self, *args):\n","          if len(self.memory) < self.capacity:\n","              self.memory.append(None)\n","          self.memory[self.position] = Transition(*args)\n","          self.position = (self.position + 1) % self.capacity\n","\n","      def sample(self, batch_size):\n","          return random.sample(self.memory, batch_size)\n","\n","      def __len__(self):\n","          return len(self.memory)\n","\n","  class PolicyNetworkDQN(nn.Module):\n","      def __init__(self, state_size, action_size, hidden_size=128):\n","          super(PolicyNetworkDQN, self).__init__()\n","          layers = [\n","                nn.Linear(state_size, hidden_size),\n","                nn.ReLU(),\n","                nn.Linear(hidden_size, hidden_size),\n","                nn.ReLU(),\n","                nn.Linear(hidden_size, hidden_size),\n","                nn.ReLU(),\n","                nn.Linear(hidden_size, action_size)\n","          ]\n","          self.model = nn.Sequential(*layers)\n","\n","      def forward(self, x):\n","          q_values = self.model(x)\n","          return q_values  \n","\n","  class AnnealedEpsGreedyPolicy(object):\n","      def __init__(self, eps_start = 0.9, eps_end = 0.01, eps_decay = 400):\n","          self.eps_start = eps_start\n","          self.eps_end = eps_end\n","          self.eps_decay = eps_decay\n","          self.steps_done = 0\n","\n","      def select_action(self, q_values):\n","          sample = random.random()\n","          eps_threshold = self.eps_end + (self.eps_start - self.eps_end) * math.exp(-1. * self.steps_done / self.eps_decay)\n","          self.steps_done += 1\n","          if sample > eps_threshold:\n","              return np.argmax(q_values)\n","          else:\n","              return random.randrange(len(q_values))\n","\n","  GAMMA = 0.8\n","  TARGET_UPDATE = 20\n","  BATCH_SIZE = 512\n","\n","  def update_model(memory, policy_net, target_net):\n","      if len(memory) < BATCH_SIZE:\n","          return\n","      transitions = memory.sample(BATCH_SIZE)\n","      batch = Transition(*zip(*transitions))\n","\n","      non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n","      non_final_next_states = torch.stack([s for s in batch.next_state if s is not None])\n","\n","      state_batch = torch.stack(batch.state)\n","      action_batch = torch.cat(batch.action)\n","      reward_batch = torch.stack(batch.reward)\n","\n","      state_action_values = policy_net(state_batch).gather(1, action_batch)\n","\n","      next_state_values = torch.zeros(BATCH_SIZE, device=device)\n","      next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n","\n","      # Compute the expected Q values\n","      expected_state_action_values = reward_batch[:, 0] + (GAMMA * next_state_values)  \n","\n","      # Compute Huber loss\n","      loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n","      \n","      # Optimize the model\n","      optimizer.zero_grad()\n","      loss.backward()\n","      for param in policy_net.parameters():\n","          param.grad.data.clamp_(-1, 1)\n","      optimizer.step()\n","\n","  def env_intial_state():\n","      a=np.repeat(0, 2*T)\n","      a[0]=df['price_per_unit'].iloc[-1]\n","      return a\n","\n","  def env_step(t, state, action,p_min,p_avg):\n","      next_state = np.repeat(0, len(state))\n","      next_state[0] = price_grid[action]\n","      next_state[1:T] = state[0:T-1]\n","      \n","      next_state[T+t] = 1\n","   \n","      reward = profit_t_response(next_state[0], next_state[1],p_min,p_avg)\n","      return next_state, reward\n","\n","  def to_tensor(x):\n","      return torch.from_numpy(np.array(x).astype(np.float32))\n","\n","  def to_tensor_long(x):\n","      return torch.tensor([[x]], device=device, dtype=torch.long)\n","\n","  policy_net = PolicyNetworkDQN(2*T, len(price_grid)).to(device)\n","  target_net = PolicyNetworkDQN(2*T, len(price_grid)).to(device)\n","  optimizer = optim.AdamW(policy_net.parameters(), lr = 0.005)\n","  policy = AnnealedEpsGreedyPolicy()\n","  memory = ReplayMemory(10000)\n","\n","  target_net.load_state_dict(policy_net.state_dict())\n","  target_net.eval()\n","\n","  num_episodes = 500\n","  return_trace = []\n","  p_trace = [] # price schedules used in each episode\n","  for i_episode in range(num_episodes):\n","      state = env_intial_state()\n","      #print(state)\n","      reward_trace = []\n","      p = []\n","      q=[]\n","      for t in range(T): \n","          forecasted_sales_t=forecasts[t][0]#############\n","          # Select and perform an action\n","          with torch.no_grad():\n","              q_values = policy_net(to_tensor(state))\n","          action = policy.select_action(q_values.detach().numpy())\n","          next_state, reward = env_step(t, state, action,p_min,p_avg)\n","\n","          # Store the transition in memory\n","          memory.push(to_tensor(state), \n","                      to_tensor_long(action), \n","                      to_tensor(next_state) if t != T - 1 else None, \n","                      to_tensor([reward]))\n","\n","          # Move to the next state\n","          state = next_state\n","\n","          # Perform one step of the optimization (on the target network)\n","          update_model(memory, policy_net, target_net)\n","\n","          reward_trace.append(reward)\n","          p.append(price_grid[action])\n","          #q.append(q_t(p_t, p_t_1,elasticity,forecasted_sales_t))\n","\n","      return_trace.append(sum(reward_trace))\n","      p_trace.append(p)\n","\n","      # Update the target network, copying all weights and biases in DQN\n","      if i_episode % TARGET_UPDATE == 0:\n","          target_net.load_state_dict(policy_net.state_dict())\n","\n","          #clear_output(wait = True)\n","          #print(f'Episode {i_episode} of {num_episodes} ({i_episode/num_episodes*100:.2f}%)')\n","\n","  max_profit=np.amax(return_trace)\n","  max_profit_index=np.argmax(return_trace)\n","\n","\n","  if plot_graphs==True:\n","    plot_return_trace(return_trace,T)\n","    fig = plt.figure(figsize=(16, 5))\n","    plot_price_schedules(p_trace, 5, 1,T)\n","\n","  plt.show()\n","\n","  return  p_trace[-2:-1]\n","\n","\n","\n","\n"]}]}